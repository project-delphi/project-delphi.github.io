<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>When Is Causality Important | Ravi Kalia Blog</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="https://project-delphi.github.io/blog/when-is-causality-important/">
<!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/latest/css/font-awesome.min.css">
<meta name="author" content="Ravi Kalia">
<link rel="prev" href="../mlops-teams-vs-tools/" title="MLOps Teams or Tools" type="text/html">
<meta property="og:site_name" content="Ravi Kalia Blog">
<meta property="og:title" content="When Is Causality Important">
<meta property="og:url" content="https://project-delphi.github.io/blog/when-is-causality-important/">
<meta property="og:description" content="A data scientist works on three types of problems:

Prediction
Description
Causality

I've been thinking of when it is that causality is important. The reason is for quite sometime I didn't put much t">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2019-09-29T16:34:44-04:00">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-expand-md static-top mb-4
navbar-dark bg-dark
"><div class="container">
<!-- This keeps the margins nice -->
        <a class="navbar-brand" href="https://project-delphi.github.io/">

            <span id="blog-title">Ravi Kalia Blog</span>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="bs-navbar">
            <ul class="navbar-nav mr-auto">
<li class="nav-item">
<a href="../../index.html" class="nav-link">Home</a>
                </li>
<li class="nav-item">
<a href="../../contact/index.html" class="nav-link">Contact</a>
                </li>
<li class="nav-item">
<a href="../../talks/index.html" class="nav-link">Talks</a>
                </li>
<li class="nav-item">
<a href="../../publications/index.html" class="nav-link">Publications</a>
                </li>
<li class="nav-item">
<a href="../../archive.html" class="nav-link">Blog</a>
                </li>
<li class="nav-item">
<a href="../../categories/" class="nav-link">Tags</a>

                
            </li>
</ul>
<ul class="navbar-nav navbar-right">
<li class="nav-item">
    <a href="index.md" id="sourcelink" class="nav-link">Source</a>
    </li>


                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        
        
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">When Is Causality Important</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                    Ravi Kalia
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2019-09-29T16:34:44-04:00" itemprop="datePublished" title="2019-09-29">2019-09-29</time></a>
            </p>
                <p class="commentline">
        
    <a href="#disqus_thread" data-disqus-identifier="cache/posts/when-is-causality-important.html">Comments</a>


            
        </p>
<p class="sourceline"><a href="index.md" class="sourcelink">Source</a></p>

        </div>
        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>A data scientist works on three types of problems:</p>
<ol>
<li>Prediction</li>
<li>Description</li>
<li>Causality</li>
</ol>
<p>I've been thinking of when it is that causality is important. The reason is for quite sometime I didn't put much thought into the area - it is messy. I had been sold on the statistical perspective that correlation is not causation, only controlled randomized experiments with enough confounders could determine causality <a href="https://arxiv.org/abs/1804.10846">[1]</a>.</p>
<p>Instead, I focussed on prediction - using regularization to press down on correlated (and possibly causal) features and weights so that deciding to leave features in or out didn't really matter. This strategy has worked combined with robustness techniques (such as bootstrap ensembling or randomized dropout) across many applications using learning algorithms.</p>
<p><img alt="" src="https://i.imgflip.com/223wf5.jpg"></p>
<p>Regularizing is as if the <del>features</del> weights on features are flattened so that only the most essential and truly impactful are allowed to affect the outcome - like a flattened statue, the essence can still be determined.</p>
<p>This prediction focus matters when there is observational data and the objective is to query a model and return a "best" prediction on a target. Indeed, much of the billions in Machine Learning have been spent on getting prediction right.</p>
<h3>When does causality matter to an organization?</h3>
<p>Well it seems that it matters when the features can be manipulated. This is also referred to as applying treatments, having controls and considering regimes.</p>
<p>More importantly, if these features have high cost (such as appliance choice in a cloud server or deployment of physical/financial resources) then it makes sense to focus on causality.</p>
<p><img alt="" src="http://www.kappit.com/img/pics/201602_1229_ihdaf_sm.jpg"></p>
<p>It depends on the context as to what cost is unbearable (reputation, dollar amount, ethical concerns) so that manipulating features becomes desirable.</p>
<p>This might be referred to as an asset allocation strategy in some fields of study.</p>
<p><img alt="" src="https://imgflip.com/i/3c0wbp"></p>
<p>One is usually looking for the best way to distribute resources based on causal inference. By estimating the counterfactual - the distribution of not applying a treatment (manipulating features) -  to optimize the target of future examples - probablistically, since deterministic targets seldom occur in business or nature.</p>
<p>This is only possible because we make assumptions on the kind of relationship that might hold going from a controlled feature vs a treatment feature to the target. There are three schools making different assumptions:</p>
<ol>
<li>Statistics using Propensity Matching</li>
<li>Computer Science using Structural Equation DAGs</li>
<li>Econometrics using Instrument Variables</li>
</ol>
<p>The group of assumptions made affect the probability distribution of the target and hence the decisions made. This in part is why causal reasoning does not have clean well understood modelling approaches, unlike prediction.</p>
<p>Different. Assumptions. Generate. Different. Distributions.</p>
</div>
    </div>
    <aside class="postpromonav"><nav><ul class="pager hidden-print">
<li class="previous">
                <a href="../mlops-teams-vs-tools/" rel="prev" title="MLOps Teams or Tools">Previous post</a>
            </li>
        </ul></nav></aside><section class="comments hidden-print"><h2>Comments</h2>
        
        
        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="https-project-delphi-github-io",
            disqus_url="https://project-delphi.github.io/blog/when-is-causality-important/",
        disqus_title="When Is Causality Important",
        disqus_identifier="cache/posts/when-is-causality-important.html",
        disqus_config = function () {
            this.language = "en";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="https://disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section></article><script>var disqus_shortname="https-project-delphi-github-io";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script><!--End of body content--><footer id="footer"><div class="text-center">
<p>
<span class="fa-stack fa-2x">
  <a href="https://twitter.com/ravkalia1">
    <i class="fa fa-circle fa-stack-2x"></i>
    <i class="fa fa-twitter fa-inverse fa-stack-1x"></i>
  </a>
</span>
<span class="fa-stack fa-2x">
  <a href="https://github.com/project-delphi">
    <i class="fa fa-circle fa-stack-2x"></i>
    <i class="fa fa-github fa-inverse fa-stack-1x"></i>
  </a>
</span>
<span class="fa-stack fa-2x">
  <a href="https://www.linkedin.com/in/ravi-kalia-phd">
    <i class="fa fa-circle fa-stack-2x"></i>
    <i class="fa fa-linkedin fa-inverse fa-stack-1x"></i>
  </a>
</span>
<span class="fa-stack fa-2x">
  <a href="mailto:ravkalia@gmail.com">
    <i class="fa fa-circle fa-stack-2x"></i>
    <i class="fa fa-envelope fa-inverse fa-stack-1x"></i>
  </a>
</span>
</p>
<p>
  Contents © 2019  <a href="mailto:ravkalia@gmail.com">Ravi Kalia</a>
  —
  
  —
  Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a>
</p>
</div>

            
        </footer>
</div>
</div>


        <script src="../../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script>
</body>
</html>
